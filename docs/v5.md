# V5 Querying System: Entity-Anchored Deep Research

## Overview

A simplified, deep-research-inspired querying system for knowledge graph QA. Follows the pattern: **Decompose → Parallel Sub-Research → Per-Subquery Synthesis → Final Synthesis**.

Key differences from V2:
- **Single execution path** (no dual modes)
- **Per-subquery answers** before final synthesis
- **Dynamic multi-hop** via LLM-guided expansion
- **Clean abstractions** with proper separation of concerns
- **~50% less code** through elimination of duplication

---

## Architecture

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           V5 PIPELINE                                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Question                                                                   │
│      │                                                                      │
│      ▼                                                                      │
│  ┌──────────────────┐                                                       │
│  │   DECOMPOSER     │  → SubQuery[], RequiredInfo[], QuestionType          │
│  │   (gpt-5.1)      │                                                       │
│  └────────┬─────────┘                                                       │
│           │                                                                 │
│           ▼                                                                 │
│  ┌──────────────────────────────────────────────────────────────────┐       │
│  │              PARALLEL SUB-QUERY RESEARCHERS                       │       │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐               │       │
│  │  │ Researcher 1│  │ Researcher 2│  │ Researcher N│   (semaphore) │       │
│  │  │             │  │             │  │             │               │       │
│  │  │ 1. Resolve  │  │ 1. Resolve  │  │ 1. Resolve  │               │       │
│  │  │ 2. Retrieve │  │ 2. Retrieve │  │ 2. Retrieve │               │       │
│  │  │ 3. Expand?  │  │ 3. Expand?  │  │ 3. Expand?  │               │       │
│  │  │ 4. Synthesize│ │ 4. Synthesize│ │ 4. Synthesize│              │       │
│  │  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘               │       │
│  │         │                │                │                       │       │
│  │         ▼                ▼                ▼                       │       │
│  │    SubAnswer_1      SubAnswer_2      SubAnswer_N                  │       │
│  └──────────────────────────────────────────────────────────────────┘       │
│           │                                                                 │
│           ▼                                                                 │
│  ┌──────────────────┐                                                       │
│  │ FINAL SYNTHESIZER│  → Combine sub-answers into final answer              │
│  │   (gpt-5.1)      │                                                       │
│  └──────────────────┘                                                       │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## File Structure

```
src/querying_system/v5/
├── __init__.py
├── pipeline.py           # Main orchestrator (~150 lines)
├── decomposer.py         # Query → SubQueries (reuse shared/)
├── researcher.py         # Per-subquery research agent (~200 lines)
├── graph_store.py        # Unified graph access layer (~250 lines)
├── schemas.py            # Data models (~100 lines)
└── prompts.py            # All LLM prompts (~150 lines)

Total: ~850 lines (vs ~2000 in V2)
```

---

## Component Design

### 1. GraphStore (Unified Graph Access Layer)

**Purpose**: Single abstraction for all Neo4j/Qdrant operations. Eliminates triplicate Cypher patterns.

```python
class GraphStore:
    """Unified interface for graph queries."""

    # Resolution
    async def resolve_entities(
        hints: list[EntityHint],
        question_context: str
    ) -> list[ResolvedEntity]

    async def resolve_topics(
        hints: list[EntityHint],
        question_context: str
    ) -> list[ResolvedTopic]

    # Retrieval (scoped)
    async def search_entity_facts(
        entity_name: str,
        query_embedding: list[float],
        threshold: float = 0.3
    ) -> list[RawFact]

    async def search_topic_facts(
        topic_name: str,
        query_embedding: list[float],
        threshold: float = 0.3
    ) -> list[RawFact]

    # Retrieval (global)
    async def search_all_facts_vector(
        query_embedding: list[float],
        top_k: int = 30
    ) -> list[RawFact]

    async def search_all_facts_keyword(
        keywords: list[str],
        top_k: int = 30
    ) -> list[RawFact]

    # Expansion
    async def expand_from_entity(
        entity_name: str,
        query_embedding: list[float],
        max_facts: int = 5
    ) -> list[RawFact]

    # Chunk content
    async def fetch_chunk_content(
        chunk_ids: list[str]
    ) -> dict[str, ChunkContent]
```

**Key design decisions**:
- All Cypher centralized in ONE file
- Returns `RawFact` (minimal struct) - scoring happens elsewhere
- Multi-tenant `group_id` baked into all queries
- Async throughout for parallel execution

---

### 2. Researcher (Per-SubQuery Agent)

**Purpose**: Handles one sub-query end-to-end, returns a `SubAnswer`.

```python
@dataclass
class SubAnswer:
    sub_query: str           # The question being answered
    answer: str              # Synthesized answer for this sub-query
    confidence: float        # 0-1 quality signal
    facts_used: list[ScoredFact]  # Evidence with provenance
    entities_found: list[str]     # For cross-query analysis

@dataclass
class ResearcherConfig:
    """Configurable flags for A/B testing."""
    enable_global_search: bool = True      # Always run global search
    enable_gap_expansion: bool = True      # LLM-guided expansion
    enable_entity_drilldown: bool = True   # ENUMERATION drill-down
    global_top_k: int = 30
    scoped_threshold: float = 0.3
    drilldown_max_entities: int = 10

class Researcher:
    """Handles one sub-query with resolve → retrieve → expand → synthesize."""

    def __init__(self, graph_store: GraphStore, config: ResearcherConfig = None):
        self.graph_store = graph_store
        self.config = config or ResearcherConfig()

    async def research(
        self,
        sub_query: SubQuery,
        question_context: str,  # Full original question for context
        question_type: QuestionType,
    ) -> SubAnswer:

        # Step 1: Resolve entities/topics from hints
        resolved = await self._resolve(sub_query.entity_hints, sub_query.topic_hints)

        # Step 2: Retrieve facts (scoped + global ALWAYS in parallel)
        facts = await self._retrieve_dual_path(resolved, sub_query.query_text)

        # Step 3: Score and check for gaps
        scored_facts, gaps = await self._score_and_identify_gaps(
            facts, sub_query.target_info
        )

        # Step 4: Dynamic expansion if gaps exist AND enabled
        if gaps and self.config.enable_gap_expansion:
            expanded = await self._expand_for_gaps(gaps, scored_facts)
            scored_facts = self._merge_and_rescore(scored_facts, expanded)

        # Step 5: Entity drill-down for ENUMERATION (configurable)
        if (question_type == QuestionType.ENUMERATION
            and self.config.enable_entity_drilldown):
            drilldown_facts = await self._entity_drilldown(scored_facts)
            scored_facts = self._merge_and_rescore(scored_facts, drilldown_facts)

        # Step 6: Synthesize sub-answer
        sub_answer = await self._synthesize(sub_query, scored_facts)

        return sub_answer

    async def _retrieve_dual_path(
        self,
        resolved: ResolvedEntities,
        query_text: str
    ) -> list[RawFact]:
        """Always runs BOTH scoped and global search in parallel."""
        query_embedding = await self._embed(query_text)
        keywords = extract_keywords(query_text)

        tasks = []

        # Scoped searches (per resolved entity/topic)
        for entity in resolved.entities:
            tasks.append(self.graph_store.search_entity_facts(
                entity.resolved_name, query_embedding, self.config.scoped_threshold
            ))
        for topic in resolved.topics:
            tasks.append(self.graph_store.search_topic_facts(
                topic.resolved_name, query_embedding, self.config.scoped_threshold
            ))

        # Global searches (ALWAYS run, even if entities resolved)
        if self.config.enable_global_search:
            tasks.append(self.graph_store.search_all_facts_vector(
                query_embedding, self.config.global_top_k
            ))
            tasks.append(self.graph_store.search_all_facts_keyword(
                keywords, self.config.global_top_k
            ))

        results = await asyncio.gather(*tasks)

        # Merge and dedupe by fact_id, apply cross-source boost
        return self._merge_facts(results)
```

**Key features**:
- **Global search always runs** in parallel with scoped (catches edge cases)
- **Entity drill-down** for ENUMERATION questions (configurable)
- **All features toggleable** via `ResearcherConfig` for A/B testing

---

### 3. Pipeline (Orchestrator)

**Purpose**: Minimal orchestration - decompose, spawn researchers, combine answers.

```python
class V5Pipeline:
    """Entity-anchored deep research pipeline."""

    def __init__(
        self,
        group_id: str = "default",
        config: ResearcherConfig = None,  # Pass through to researchers
    ):
        self.graph_store = GraphStore(group_id)
        self.decomposer = QueryDecomposer()  # Reuse from shared/
        self.config = config or ResearcherConfig()
        self.max_concurrent = 5

    async def query(self, question: str) -> PipelineResult:
        # Phase 1: Decompose
        decomposition = await self.decomposer.decompose(question)

        # Phase 2: Parallel sub-query research
        sub_answers = await self._research_parallel(
            decomposition.sub_queries,
            question_context=question
        )

        # Phase 3: Final synthesis
        final_answer = await self._synthesize_final(
            question,
            decomposition,
            sub_answers
        )

        return PipelineResult(
            question=question,
            answer=final_answer.answer,
            sub_answers=sub_answers,
            confidence=final_answer.confidence,
            evidence=self._collect_evidence(sub_answers)
        )

    async def _research_parallel(
        self,
        sub_queries: list[SubQuery],
        question_context: str
    ) -> list[SubAnswer]:
        """Run researchers in parallel with semaphore."""
        semaphore = asyncio.Semaphore(self.max_concurrent)

        async def bounded_research(sq: SubQuery) -> SubAnswer:
            async with semaphore:
                researcher = Researcher(self.graph_store)
                return await researcher.research(sq, question_context)

        return await asyncio.gather(*[
            bounded_research(sq) for sq in sub_queries
        ])
```

**~150 lines total** - the orchestrator just orchestrates.

---

### 4. Schemas (Data Models)

```python
# Input/Output
@dataclass
class SubQuery:
    query_text: str              # Focused search query
    target_info: str             # What this sub-query seeks
    entity_hints: list[str]      # Entities to resolve
    topic_hints: list[str]       # Topics to resolve

@dataclass
class SubAnswer:
    sub_query: str
    answer: str
    confidence: float
    facts_used: list[ScoredFact]
    entities_found: list[str]

@dataclass
class PipelineResult:
    question: str
    answer: str
    sub_answers: list[SubAnswer]
    confidence: float
    evidence: list[Evidence]

# Internal
@dataclass
class RawFact:
    fact_id: str
    content: str
    subject: str
    edge_type: str
    object: str
    chunk_id: str
    chunk_content: str | None
    chunk_header: str
    doc_id: str
    document_date: str
    vector_score: float
    source: str  # "scoped:EntityName", "global_vector", "global_keyword", "expansion"

@dataclass
class ScoredFact(RawFact):
    llm_relevance: float = 0.0
    cross_query_boost: float = 0.0
    final_score: float = 0.0

@dataclass
class ResolvedEntity:
    original_hint: str
    resolved_name: str
    confidence: float

@dataclass
class Evidence:
    fact_id: str
    content: str
    source_chunk: str
    source_doc: str
    document_date: str
```

---

### 5. Dynamic Multi-Hop (Gap-Driven Expansion)

**The missing piece from V2**: LLM decides when to expand, not question-type.

```python
# In Researcher._score_and_identify_gaps()

IDENTIFY_GAPS_PROMPT = """
Given these facts and the target information needed, identify gaps:

Target: {target_info}

Facts found:
{facts_summary}

Return JSON:
{{
  "gaps": [
    {{"missing": "what info is missing", "expand_from": "entity to expand from"}}
  ],
  "sufficient": true/false
}}
"""

async def _score_and_identify_gaps(
    self,
    facts: list[RawFact],
    target_info: str
) -> tuple[list[ScoredFact], list[Gap]]:

    # Score facts
    scored = await self._llm_score(facts, target_info)

    # Check for gaps (cheap LLM call)
    gap_response = await self.nano_llm.invoke(
        IDENTIFY_GAPS_PROMPT.format(
            target_info=target_info,
            facts_summary=self._summarize_facts(scored[:15])
        )
    )

    gaps = parse_gaps(gap_response)
    return scored, gaps
```

**Expansion is targeted**: Only expand from entities identified as having gaps.

---

## LLM Usage

| Step | Model | Cost | Notes |
|------|-------|------|-------|
| Decomposition | gpt-5.1 | $$ | Once per query |
| Resolution (per hint) | gpt-5-mini | $ | Parallel, cached |
| Fact scoring | gpt-5-mini | $ | Batch 30 facts |
| Gap identification | gpt-5-mini | $ | Per researcher |
| Sub-answer synthesis | gpt-5.1 | $$ | Per researcher |
| Final synthesis | gpt-5.1 | $$ | Once per query |

**Total**: 4-8 LLM calls depending on sub-queries (cheaper than V2's classic mode).

---

## Key Improvements Over V2

| Issue in V2 | V5 Solution |
|-------------|-------------|
| Two execution modes (700 lines) | Single path (~150 lines) |
| Triplicate Cypher patterns | Centralized in GraphStore |
| Facts combined directly | Per-subquery synthesis first |
| Question-type expansion | LLM-guided gap detection |
| 11 component dependencies | 4 components |
| ~2000 lines | ~850 lines |

---

## Graph Interaction Pattern

V5 respects the chunk-centric graph design:

```
Query: "What acquisitions did Apple make in Boston?"

1. Decompose → SubQuery("Apple acquisitions Boston")

2. Resolve:
   - "Apple" → EntityNode "Apple Inc."
   - "Boston" → EntityNode "Boston" (Fed district)
   - "acquisitions" → (relationship hint, not resolved)

3. Retrieve (scoped):
   MATCH (e:EntityNode {name: "Apple Inc."})-[r1]->(c:EpisodicNode)-[r2]->(target)
   WHERE r1.fact_id = r2.fact_id
   AND type(r1) CONTAINS "ACQUI"
   RETURN facts with chunk content

4. Retrieve (global):
   Vector search on "Apple acquisitions Boston"
   Keyword search on ["Apple", "acquisition", "Boston"]

5. Expand (if gaps):
   For each acquisition target entity → get related facts

6. Synthesize sub-answer:
   "Apple acquired X in Boston region..."

7. Final synthesis:
   Combine all sub-answers → final response
```

---

## Files to Modify/Create

### Create New
- `src/querying_system/v5/__init__.py`
- `src/querying_system/v5/pipeline.py`
- `src/querying_system/v5/researcher.py`
- `src/querying_system/v5/graph_store.py`
- `src/querying_system/v5/schemas.py`
- `src/querying_system/v5/prompts.py`

### Reuse from Shared
- `src/querying_system/shared/decomposer.py` (import QueryDecomposer)
- `src/querying_system/shared/schemas.py` (import EntityHint, QuestionType)

### No Modifications to Existing
- V2 remains untouched for A/B testing

---

## Verification Plan

1. **Unit tests**: Test each component in isolation
   - `tests/test_graph_store.py` - Mock Neo4j, verify Cypher
   - `tests/test_researcher.py` - Mock graph_store, verify flow
   - `tests/test_pipeline.py` - Mock researcher, verify orchestration

2. **Integration test**: Run against real graph
   ```bash
   uv run python -c "
   from src.querying_system.v5.pipeline import V5Pipeline
   import asyncio

   async def test():
       p = V5Pipeline()
       result = await p.query('What economic conditions did Boston report?')
       print(result.answer)
       print(f'Confidence: {result.confidence}')
       for sa in result.sub_answers:
           print(f'  Sub: {sa.sub_query} -> {sa.answer[:100]}...')

   asyncio.run(test())
   "
   ```

3. **Comparison eval**: Run same questions through V2 and V5
   ```bash
   uv run python scripts/compare_v2_v5.py --questions eval/test_questions.json
   ```

4. **Metrics to track**:
   - Answer accuracy (judge scoring)
   - Latency (should be similar or better due to parallelism)
   - LLM cost (should be lower)
   - Code complexity (lines of code)

5. **A/B testing configurations**:
   ```python
   # scripts/eval_v5_ablation.py

   CONFIGS = {
       "full": ResearcherConfig(
           enable_global_search=True,
           enable_gap_expansion=True,
           enable_entity_drilldown=True,
       ),
       "no_drilldown": ResearcherConfig(
           enable_global_search=True,
           enable_gap_expansion=True,
           enable_entity_drilldown=False,  # Disable drill-down
       ),
       "no_gap_expansion": ResearcherConfig(
           enable_global_search=True,
           enable_gap_expansion=False,  # Disable LLM-guided expansion
           enable_entity_drilldown=True,
       ),
       "scoped_only": ResearcherConfig(
           enable_global_search=False,  # No global fallback
           enable_gap_expansion=True,
           enable_entity_drilldown=True,
       ),
       "minimal": ResearcherConfig(
           enable_global_search=True,
           enable_gap_expansion=False,
           enable_entity_drilldown=False,
       ),
   }

   # Run eval for each config
   for name, config in CONFIGS.items():
       pipeline = V5Pipeline(config=config)
       results = await run_eval(pipeline, questions)
       print(f"{name}: accuracy={results.accuracy}, latency={results.avg_latency}ms")
   ```

   This lets you measure the impact of each feature on accuracy.

---

## Implementation Order

1. **schemas.py** - Define data models first
2. **prompts.py** - All LLM prompts in one place
3. **graph_store.py** - Core graph abstraction (can test independently)
4. **researcher.py** - Per-subquery agent (depends on graph_store)
5. **pipeline.py** - Orchestrator (depends on researcher)
6. **Tests** - Unit + integration
7. **Eval script** - Compare V2 vs V5
