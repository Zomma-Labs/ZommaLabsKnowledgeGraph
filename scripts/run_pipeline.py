import sys
import os

# Add project root to sys.path to allow importing from src
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import asyncio
import json
from typing import Dict
from src.workflows.main_pipeline import app
from src.scripts.setup_graph_index import setup_index

# Path to saved chunks
CHUNKS_DIR = "src/chunker/SAVED"

async def main():
    print("ğŸš€ Starting Pipeline Run...")
    
    # Ensure Vector Indices Exist
    print("\nâš™ï¸ Setting up Vector Indices...")
    setup_index()
    print("âœ… Indices Ready.\n")
    
    # 1. Load Chunks
    # We'll look for .jsonl files in the SAVED directory
    # Filter for Beige Book only as requested
    chunk_files = [f for f in os.listdir(CHUNKS_DIR) if f.endswith('.jsonl') and ('alphabet' in f.lower() or 'berkshire' in f.lower() or 'largest' in f.lower())]
    
    if not chunk_files:
        print("âŒ No chunk files found in src/chunker/SAVED")
        return

    print(f"ğŸ“‚ Found {len(chunk_files)} chunk files: {chunk_files}")
    
    tasks = []
    
    for filename in chunk_files:
        filepath = os.path.join(CHUNKS_DIR, filename)
        print(f"\nğŸ“„ Processing file: {filename}")
        
        with open(filepath, 'r') as f:
            # Read line by line (each line is a chunk object)
            # Limit to first few chunks for testing to avoid huge costs/time
            for i, line in enumerate(f):
                if i >= 39: # Limit to 3 chunks per file for test
                    break
                
                chunk_data = json.loads(line)
                
                # Extract text and metadata
                # Based on standard chunker output:
                chunk_text = chunk_data.get("body", "")
                metadata = chunk_data.get("metadata", {})
                # Inject doc_id into metadata for consistent document identification
                if "doc_id" in chunk_data:
                    metadata["doc_id"] = chunk_data["doc_id"]
                
                if not chunk_text:
                    print(f"   âš ï¸ Skipping chunk {i}: No text found.")
                    continue
                    
                # Ensure headings/breadcrumbs are in metadata
                if "headings" not in metadata and "breadcrumbs" in chunk_data:
                    metadata["headings"] = chunk_data["breadcrumbs"]

                # Initialize State
                initial_state = {
                    "chunk_text": chunk_text,
                    "metadata": metadata,
                    "group_id": "default_tenant", # Default tenant
                    "episodic_uuid": "", # Will be generated by pipeline
                    "atomic_facts": [],
                    "resolved_entities": [],
                    "classified_relationships": [],
                    "causal_links": [],
                    "errors": []
                }
                
                # Create Task
                tasks.append(process_chunk(i, initial_state))

    # Run all chunks in parallel
    if tasks:
        print(f"   ğŸš€ Running {len(tasks)} chunks in parallel...")
        await asyncio.gather(*tasks)

async def process_chunk(i: int, state: Dict):
    print(f"   ğŸ”„ Starting Chunk {i}...")
    try:
        # app.ainvoke is async
        result = await app.ainvoke(state)
        
        if result.get("errors"):
            print(f"   âŒ Chunk {i} Errors: {result['errors']}")
        else:
            print(f"   âœ… Chunk {i} Completed Successfully.")
            
    except Exception as e:
        print(f"   âŒ Chunk {i} Failed: {e}")

if __name__ == "__main__":
    asyncio.run(main())
